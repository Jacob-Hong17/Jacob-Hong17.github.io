# jemdoc: menu{MENU}{index.html}, footer
==Bayesian Ferderated Learning

Current data-driven artificial intelligence algorithms are extremely dependent on the size of the training data. The edge 
computing device, such as smartphones and IoT devices, have a lot of data suitable for training models. However, due to current 
legal restrictions and public concerns about privacy, we are unable to access user data directly. To address this problem, 
Federal Learning focuses on training powerful models without accessing client data.\n

\n
There are many problems facing the current federal learning application process. One of them is that client data is not 
Independently Identically Distributed (non-i.i.d.). The current popular algorithm for federation learning, FEDAVG, is unable to 
perform in this case and is still some distance away from being practical.\n

\n
When we analyze the FEDAVG algorithm, we can find that FEDAVG is inherently unable to handle the disagreement in the clients’ 
models because FEDAVG discards the variance information of the clients’ models in the aggregation parameter stage, which 
reflects the disagreement among clients created by non-i.i.d. dataset.\n

\n
The difference between a Bayesian neural network (BNN) and a point estimation neural network is that a BNN treats the 
parameters as distributions rather than as some fixed value. The distribution will be more informative compared to a single 
point. Therefore BNN can effectively preserve the disagreement between client models. Briefly BNNs will allow client models to 
explore alternative possibilities and thus not be limited to the optimum generated by their own database.\n

~~~
{}{img_left}{project/bfl.png}{alt text}{433}{160}{} \n 
~~~



